#!/usr/bin/env python3
"""
PostgreSQL Migration Tool –¥–ª—è Telegram Mini App
–ü–µ—Ä–µ–Ω–æ—Å –¥–∞–Ω–Ω—ã—Ö –∏–∑ SQLite –≤ PostgreSQL —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
"""

import os
import sys
import sqlite3
import psycopg2
import psycopg2.extras
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
import json

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.config.telegram_config import AppConfig

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('migration.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class DatabaseMigrator:
    """–ö–ª–∞—Å—Å –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ SQLite –≤ PostgreSQL"""
    
    def __init__(self):
        self.sqlite_path = AppConfig.DATABASE_PATH
        self.postgres_url = os.environ.get(
            'DATABASE_URL', 
            'postgresql://telegram_user:secure_password_2025@localhost:5432/telegram_mini_app'
        )
        
        # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∞–±–ª–∏—Ü –∏ –∏—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
        self.table_mappings = {
            'users': {
                'columns': [
                    'id', 'telegram_id', 'username', 'first_name', 'last_name',
                    'phone_number', 'balance', 'is_admin', 'is_active', 
                    'language_code', 'timezone', 'created_at', 'updated_at', 'last_login'
                ],
                'id_column': 'id'
            },
            'channels': {
                'columns': [
                    'id', 'owner_id', 'title', 'username', 'description', 'category',
                    'subscriber_count', 'price_per_post', 'is_verified', 'is_active', 
                    'verification_code', 'verification_expires_at', 'last_stats_update',
                    'created_at', 'updated_at'
                ],
                'id_column': 'id'
            },
            'offers': {
                'columns': [
                    'id', 'created_by', 'advertiser_id', 'title', 'description', 
                    'target_audience', 'budget_total', 'price', 'currency', 'status',
                    'category', 'placement_type', 'start_date', 'end_date',
                    'created_at', 'updated_at', 'expires_at'
                ],
                'id_column': 'id'
            },
            'offer_responses': {
                'source_table': 'offer_proposals',  # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
                'columns': [
                    'id', 'offer_id', 'channel_id', 'user_id', 'status', 
                    'response_message', 'proposed_price', 'counter_offers_count',
                    'created_at', 'updated_at', 'responded_at', 'expires_at'
                ],
                'id_column': 'id'
            },
            'offer_placements': {
                'columns': [
                    'id', 'response_id', 'status', 'post_url', 
                    'placement_start', 'placement_end', 'views_count', 'clicks_count',
                    'engagement_rate', 'created_at', 'updated_at'
                ],
                'id_column': 'id'
            },
            'payments': {
                'columns': [
                    'id', 'user_id', 'offer_id', 'placement_id', 'amount', 'currency',
                    'payment_type', 'status', 'provider', 'provider_payment_id',
                    'description', 'created_at', 'updated_at', 'processed_at'
                ],
                'id_column': 'id'
            },
            'analytics_events': {
                'columns': [
                    'id', 'user_id', 'channel_id', 'offer_id', 'event_type',
                    'event_data', 'session_id', 'ip_address', 'user_agent', 'created_at'
                ],
                'id_column': 'id'
            },
            'notifications': {
                'columns': [
                    'id', 'user_id', 'title', 'message', 'notification_type',
                    'is_read', 'data', 'created_at', 'read_at'
                ],
                'id_column': 'id'
            }
        }
    
    def connect_sqlite(self) -> sqlite3.Connection:
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ SQLite"""
        try:
            conn = sqlite3.connect(self.sqlite_path)
            conn.row_factory = sqlite3.Row
            logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ SQLite: {self.sqlite_path}")
            return conn
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ SQLite: {e}")
            raise
    
    def connect_postgres(self) -> psycopg2.extensions.connection:
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL"""
        try:
            conn = psycopg2.connect(self.postgres_url)
            conn.autocommit = False
            logger.info(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL")
            return conn
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL: {e}")
            raise
    
    def run_sql_file(self, pg_conn: psycopg2.extensions.connection, sql_file: str):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ SQL —Ñ–∞–π–ª–∞"""
        try:
            with open(sql_file, 'r', encoding='utf-8') as f:
                sql_content = f.read()
            
            cursor = pg_conn.cursor()
            cursor.execute(sql_content)
            pg_conn.commit()
            logger.info(f"‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω SQL —Ñ–∞–π–ª: {sql_file}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è SQL —Ñ–∞–π–ª–∞ {sql_file}: {e}")
            pg_conn.rollback()
            raise
    
    def check_table_exists(self, sqlite_conn: sqlite3.Connection, table_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –≤ SQLite"""
        cursor = sqlite_conn.cursor()
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name=?
        """, (table_name,))
        return cursor.fetchone() is not None
    
    def get_table_data(self, sqlite_conn: sqlite3.Connection, table_name: str, columns: List[str]) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü—ã SQLite"""
        try:
            cursor = sqlite_conn.cursor()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ —Å—Ç–æ–ª–±—Ü—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç
            cursor.execute(f"PRAGMA table_info({table_name})")
            existing_columns = [row[1] for row in cursor.fetchall()]
            
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã
            valid_columns = [col for col in columns if col in existing_columns]
            
            if not valid_columns:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {table_name}")
                return []
            
            columns_str = ', '.join(valid_columns)
            cursor.execute(f"SELECT {columns_str} FROM {table_name}")
            
            rows = cursor.fetchall()
            result = []
            
            for row in rows:
                item = {}
                for i, col in enumerate(valid_columns):
                    value = row[i]
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ JSON –ø–æ–ª–µ–π
                    if col in ['event_data', 'data', 'details'] and isinstance(value, str):
                        try:
                            item[col] = json.loads(value)
                        except:
                            item[col] = value
                    else:
                        item[col] = value
                
                result.append(item)
            
            logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–æ {len(result)} –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ç–∞–±–ª–∏—Ü—ã {table_name}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ {table_name}: {e}")
            return []
    
    def insert_data_to_postgres(self, pg_conn: psycopg2.extensions.connection, 
                               table_name: str, data: List[Dict], columns: List[str]):
        """–í—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL"""
        if not data:
            logger.info(f"‚è≠Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ {table_name}")
            return
        
        try:
            cursor = pg_conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            values_list = []
            for item in data:
                values = []
                for col in columns:
                    value = item.get(col)
                    
                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è JSONB –ø–æ–ª–µ–π
                    if col in ['event_data', 'data', 'details'] and value is not None:
                        if isinstance(value, (dict, list)):
                            value = json.dumps(value)
                    
                    values.append(value)
                
                values_list.append(values)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
            placeholders = ', '.join(['%s'] * len(columns))
            columns_str = ', '.join(columns)
            
            query = f"""
                INSERT INTO {table_name} ({columns_str}) 
                VALUES ({placeholders})
                ON CONFLICT DO NOTHING
            """
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º batch insert
            cursor.executemany(query, values_list)
            pg_conn.commit()
            
            logger.info(f"‚úÖ –í—Å—Ç–∞–≤–ª–µ–Ω–æ {cursor.rowcount} –∑–∞–ø–∏—Å–µ–π –≤ {table_name}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—Å—Ç–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ {table_name}: {e}")
            pg_conn.rollback()
            raise
    
    def migrate_table(self, sqlite_conn: sqlite3.Connection, 
                     pg_conn: psycopg2.extensions.connection, 
                     target_table: str, mapping: Dict):
        """–ú–∏–≥—Ä–∞—Ü–∏—è –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã"""
        logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –º–∏–≥—Ä–∞—Ü–∏—é —Ç–∞–±–ª–∏—Ü—ã {target_table}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
        source_table = mapping.get('source_table', target_table)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
        if not self.check_table_exists(sqlite_conn, source_table):
            logger.warning(f"‚ö†Ô∏è –¢–∞–±–ª–∏—Ü–∞ {source_table} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ SQLite")
            return
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = self.get_table_data(sqlite_conn, source_table, mapping['columns'])
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        if data:
            self.insert_data_to_postgres(pg_conn, target_table, data, mapping['columns'])
        
        logger.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã {target_table} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
    def update_sequences(self, pg_conn: psycopg2.extensions.connection):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π PostgreSQL"""
        try:
            cursor = pg_conn.cursor()
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã —Å SERIAL —Å—Ç–æ–ª–±—Ü–∞–º–∏
            cursor.execute("""
                SELECT schemaname, tablename, columnname, sequencename
                FROM pg_sequences 
                WHERE schemaname = 'public'
            """)
            
            sequences = cursor.fetchall()
            
            for schema, table, column, sequence in sequences:
                cursor.execute(f"""
                    SELECT setval('{sequence}', 
                           COALESCE((SELECT MAX({column}) FROM {table}), 1))
                """)
                
                logger.info(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å {sequence}")
            
            pg_conn.commit()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {e}")
            pg_conn.rollback()
    
    def verify_migration(self, sqlite_conn: sqlite3.Connection, 
                        pg_conn: psycopg2.extensions.connection):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –º–∏–≥—Ä–∞—Ü–∏–∏"""
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –º–∏–≥—Ä–∞—Ü–∏–∏")
        
        cursor_pg = pg_conn.cursor()
        cursor_sqlite = sqlite_conn.cursor()
        
        errors = []
        
        for table_name, mapping in self.table_mappings.items():
            source_table = mapping.get('source_table', table_name)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
            if self.check_table_exists(sqlite_conn, source_table):
                cursor_sqlite.execute(f"SELECT COUNT(*) FROM {source_table}")
                sqlite_count = cursor_sqlite.fetchone()[0]
                
                cursor_pg.execute(f"SELECT COUNT(*) FROM {table_name}")
                pg_count = cursor_pg.fetchone()[0]
                
                if sqlite_count != pg_count:
                    error_msg = f"‚ùå {table_name}: SQLite={sqlite_count}, PostgreSQL={pg_count}"
                    errors.append(error_msg)
                    logger.error(error_msg)
                else:
                    logger.info(f"‚úÖ {table_name}: {pg_count} –∑–∞–ø–∏—Å–µ–π")
        
        if errors:
            logger.error("‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—à–∏–±–∫–∏ –º–∏–≥—Ä–∞—Ü–∏–∏:")
            for error in errors:
                logger.error(f"  {error}")
            return False
        else:
            logger.info("‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ!")
            return True
    
    def run_migration(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –º–∏–≥—Ä–∞—Ü–∏–∏"""
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –º–∏–≥—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –∏–∑ SQLite –≤ PostgreSQL")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ SQLite —Ñ–∞–π–ª–∞
        if not os.path.exists(self.sqlite_path):
            logger.error(f"‚ùå SQLite —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.sqlite_path}")
            return False
        
        try:
            # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö
            sqlite_conn = self.connect_sqlite()
            pg_conn = self.connect_postgres()
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –º–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã
            migrations_dir = os.path.join(os.path.dirname(__file__), 'migrations')
            
            migration_files = [
                '001_initial_schema.sql',
                '002_performance_optimizations.sql'
            ]
            
            for migration_file in migration_files:
                file_path = os.path.join(migrations_dir, migration_file)
                if os.path.exists(file_path):
                    self.run_sql_file(pg_conn, file_path)
                else:
                    logger.warning(f"‚ö†Ô∏è –ú–∏–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            
            # –ú–∏–≥—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º
            for table_name, mapping in self.table_mappings.items():
                self.migrate_table(sqlite_conn, pg_conn, table_name, mapping)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            self.update_sequences(pg_conn)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –º–∏–≥—Ä–∞—Ü–∏–∏
            success = self.verify_migration(sqlite_conn, pg_conn)
            
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
            sqlite_conn.close()
            pg_conn.close()
            
            if success:
                logger.info("üéâ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
                
                # –°–æ–∑–¥–∞–µ–º backup SQLite —Ñ–∞–π–ª–∞
                backup_path = f"{self.sqlite_path}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                os.rename(self.sqlite_path, backup_path)
                logger.info(f"üíæ SQLite —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ backup: {backup_path}")
                
                return True
            else:
                logger.error("‚ùå –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–∞–º–∏")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –º–∏–≥—Ä–∞—Ü–∏–∏: {e}")
            return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    migrator = DatabaseMigrator()
    
    # –°–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
    print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –º–∏–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ SQLite –≤ PostgreSQL")
    print(f"üìÅ SQLite: {migrator.sqlite_path}")
    print(f"üêò PostgreSQL: {migrator.postgres_url}")
    print()
    
    confirm = input("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –º–∏–≥—Ä–∞—Ü–∏—é? (yes/no): ").lower().strip()
    
    if confirm in ['yes', 'y', '–¥–∞', '–¥']:
        success = migrator.run_migration()
        sys.exit(0 if success else 1)
    else:
        print("‚ùå –ú–∏–≥—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
        sys.exit(0)

if __name__ == "__main__":
    main()